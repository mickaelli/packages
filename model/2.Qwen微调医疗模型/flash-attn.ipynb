{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flash Attention使用示例\n",
    "\n",
    "这里演示了如何在项目中使用`flash-attn`的官方实现。\n",
    "\n",
    "首先按照官方文档的说明，安装`flash-attn`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch packaging ninja wheel\n",
    "%pip install flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self Attention的pytorch实现\n",
    "\n",
    "我们使用pytorch实现了一个self-attention函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def torch_attention(q, k, v, mask=None):\n",
    "    '''\n",
    "    PyTorch implementation of the scaled dot-product attention mechanism.\n",
    "    Parameters:\n",
    "        q: query tensor, [batch_size, n_heads, seq_len, hidden_size]\n",
    "        k: key tensor, [batch_size, n_heads, seq_len, hidden_size]\n",
    "        v: value tensor, [batch_size, n_heads, seq_len, hidden_size]\n",
    "        mask: mask tensor, [batch_size, n_heads, seq_len, seq_len]\n",
    "    Returns:\n",
    "        attention output: output tensor, [batch_size, n_heads, seq_len, hidden_size]\n",
    "    '''\n",
    "    hidden_size = q.size(-1)\n",
    "    scores = torch.matmul(q, k.transpose(-2, -1)) / torch.sqrt(torch.tensor(hidden_size, dtype=torch.float16)) # [batch_size, n_heads, seq_len, seq_len]\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    weights = torch.nn.functional.softmax(scores, dim=-1)\n",
    "    return torch.matmul(weights, v) # [batch_size, n_heads, seq_len, hidden_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于一些随机向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "batch_size = 4\n",
    "n_heads = 2\n",
    "seq_len = [2, 3, 8, 4]\n",
    "hidden_size = 32\n",
    "\n",
    "q = torch.randn(batch_size, n_heads, max(seq_len), hidden_size, dtype=torch.float16, device='cuda')\n",
    "k = torch.randn(batch_size, n_heads, max(seq_len), hidden_size, dtype=torch.float16, device='cuda')\n",
    "v = torch.randn(batch_size, n_heads, max(seq_len), hidden_size, dtype=torch.float16, device='cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分别使用pytorch实现和`flash_attn`库中所提供的attention实现计算注意力机制的输出。并且比较两个输出的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "tensor([-0.9575,  2.1211, -0.0684, -0.7085, -0.3413, -0.1348, -0.7061, -0.9697,\n",
      "         0.2690,  0.6123, -0.9048, -0.1287, -0.2888,  0.5713, -0.3391,  1.0098,\n",
      "         0.8198, -0.0552, -0.4216,  0.5845, -0.6831,  0.3074,  0.5024, -0.5537,\n",
      "         0.8457, -0.6489, -0.6060, -1.1484, -0.3499, -0.2136,  0.6978,  0.2974],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "tensor([-0.9580,  2.1211, -0.0685, -0.7090, -0.3416, -0.1349, -0.7061, -0.9707,\n",
      "         0.2690,  0.6123, -0.9058, -0.1292, -0.2888,  0.5718, -0.3398,  1.0107,\n",
      "         0.8198, -0.0551, -0.4216,  0.5850, -0.6831,  0.3079,  0.5029, -0.5537,\n",
      "         0.8457, -0.6494, -0.6060, -1.1494, -0.3499, -0.2140,  0.6982,  0.2974],\n",
      "       device='cuda:0', dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from flash_attn import flash_attn_func\n",
    "\n",
    "torch_attention_output = torch_attention(q, k, v)\n",
    "flash_attn_output = flash_attn_func(\n",
    "    q.transpose(1, 2), \n",
    "    k.transpose(1, 2), \n",
    "    v.transpose(1, 2)).transpose(1, 2)\n",
    "\n",
    "def compare_tensors(t1, t2):\n",
    "    return torch.allclose(t1, t2, atol=1e-3)\n",
    "\n",
    "print(compare_tensors(torch_attention_output, flash_attn_output))\n",
    "print(torch_attention_output[0][0][0])\n",
    "print(flash_attn_output[0][0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上述计算结果可以看出，在半精度浮点数下，使用两种实现得到的attention计算结果是一致的。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
