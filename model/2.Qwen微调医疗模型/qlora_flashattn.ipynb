{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用QLoRA 和 FlashAttention微调Qwen模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，安装必要的环境。如果没有安装flash-attn的话需要按照官方文档的说明安装flash-attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -i https://mirrors.tuna.tsinghua.edu.cn/pypi/web/simple transformers=='4.45.2' peft=='0.13.1' accelerate=='1.0.0' tiktoken bitsandbytes datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "导入需要使用的各个库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"    # 这里设置使用哪块GPU\n",
    "from torch.utils.data import Dataset        # 用于自定义数据集\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,                   # 用于加载预训练模型\n",
    "    AutoTokenizer,                          # 用于加载分词器\n",
    "    BitsAndBytesConfig,                     # 用于加载配置文件\n",
    "    TrainingArguments,                      # 用于加载训练参数\n",
    "    Trainer,                                # 用于训练\n",
    "    DataCollatorForLanguageModeling,        # 用于处理数据\n",
    ")\n",
    "import torch\n",
    "import time\n",
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(dataset): 101602\n",
      "{'department': '营养保健科',\n",
      " 'input': '女宝宝，刚7岁，这一年，察觉到，我家孩子身上肉很多，而且，食量非常的大，平时都不喜欢吃去玩，请问：小儿肥胖超重该如何治疗。',\n",
      " 'output': '孩子出现肥胖症的情况。家长要通过孩子运功和健康的饮食来缓解他的症状，可以先让他做一些有氧运动，比如慢跑，爬坡，游泳等，并且饮食上孩子多吃黄瓜，胡萝卜，菠菜等，禁止孩子吃一些油炸食品和干果类食物，这些都是干热量高脂肪的食物，而且不要让孩子总是吃完就躺在床上不动，家长在治疗小儿肥胖期间如果孩子情况严重就要及时去医院在医生的指导下给孩子治疗。'}\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import pprint\n",
    "def load_dataset(filename):\n",
    "    data_list = []\n",
    "    # read csv\n",
    "    with open(filename, \"r\", encoding=\"gb18030\") as f:    # 这里使用gb18030编码，因为我们所选择的数据csv文件是这种编码\n",
    "        reader = csv.DictReader(f)                        # 使用csv.DictReader读取csv文件\n",
    "        for row in reader:\n",
    "            data_list.append(\n",
    "                {\n",
    "                    'department': row['department'],\n",
    "                    'input': row['ask'],\n",
    "                    'output': row['answer']\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return data_list                  \n",
    "                       \n",
    "dataset = load_dataset(\"儿科5-14000.csv\")                  # 读取数据\n",
    "print('len(dataset):', len(dataset))\n",
    "pprint.pprint(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "准备可以用来模型训练的数据集，包括医疗对话数据和自我认知数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'identity_547', 'conversations': [{'from': 'user', 'value': '男孩，8岁，上小学了，刚开始，说话时觉得嗓子疼，察觉到，咳嗽好像也比较严重，而且，有点发烧也没精神，请问：孩子出现扁桃体炎哭闹饮食禁忌是什么。'}, {'from': 'assistant', 'value': '发现扁桃体炎务必要及时给孩子治疗，另外日常饮食可以多吃一些蔬菜和水果，主要是因为蔬菜和水果不但清淡，而且还富含人体所需的各种维生素和微量元素，显然是可以帮助孩子抵御疾病的，水果的话建议选择梨、苹果等，蔬菜的话像青菜、白菜、西红柿等都是不错的，可以根据孩子的喜好选择，日常饮食应选择易于消化的，比如粥、面条等，在清淡饮食的同时还要少吃发物，例如家禽蛋、驴肉、牛羊肉等，饮食对扁桃体炎的治疗也是很关键的，务必注意。'}]}\n"
     ]
    }
   ],
   "source": [
    "def prepare_message(data_list):\n",
    "    new_list = []\n",
    "    for i, data in enumerate(data_list):\n",
    "        _id = f\"identity_{i}\"\n",
    "        new_list.append(\n",
    "        {\n",
    "            \"id\": _id,\n",
    "            \"conversations\": [       # 将所输入的对话转换成conversations的形式\n",
    "                {\n",
    "                    \"from\": \"user\",\n",
    "                    \"value\": data[\"input\"]\n",
    "                },\n",
    "                {\n",
    "                    \"from\": \"assistant\",\n",
    "                    \"value\": data[\"output\"]\n",
    "                }\n",
    "            ]\n",
    "        })\n",
    "    return new_list   \n",
    "\n",
    "\n",
    "def replace_name(s):\n",
    "    s = s.replace('<NAME>', '智能医生客服机器人小D')\n",
    "    s = s.replace('<AUTHOR>', 'Greedy AI')\n",
    "    return s\n",
    "\n",
    "\n",
    "def load_self_cong_data(filename):\n",
    "    data_list = []\n",
    "    id = 0\n",
    "    for d in json.load(open(filename, \"r\", encoding=\"utf-8\")):\n",
    "        d[\"instruction\"] = replace_name(d[\"instruction\"])     # 将instruction中的<NAME>和<AUTHOR>替换成智能医生客服机器人小D和Greedy AI\n",
    "        d[\"output\"] = replace_name(d[\"output\"])\n",
    "        data_list.append({\n",
    "            \"id\": id,\n",
    "            \"conversations\": [\n",
    "                {\n",
    "                    \"from\": \"user\",\n",
    "                    \"value\": d[\"instruction\"]\n",
    "                },\n",
    "                {\n",
    "                    \"from\": \"assistant\",\n",
    "                    \"value\": d[\"output\"]\n",
    "                }\n",
    "            ]\n",
    "        })\n",
    "        id += 1\n",
    "    return data_list\n",
    "\n",
    "self_cong_data = load_self_cong_data(\"self_cognition.json\")       # 读取自我认知数据\n",
    "format_data_list = prepare_message(dataset[:1000])                # 将数据转换成conversations的形式\n",
    "format_data_list = self_cong_data + format_data_list\n",
    "random.shuffle(format_data_list)                                  # 打乱数据\n",
    "print(format_data_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实现数据加载脚本，将数据通过tokenizer转换成模型可以接受的token id输入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatting inputs...\n",
      "Formatting done...\n",
      "{'input_ids': tensor([151644,   8948,    198,  ..., 151643, 151643, 151643],\n",
      "       dtype=torch.int32), 'labels': tensor([151644,   -100,   -100,  ...,   -100,   -100,   -100],\n",
      "       dtype=torch.int32), 'attention_mask': tensor([ True,  True,  True,  ..., False, False, False])}\n"
     ]
    }
   ],
   "source": [
    "model_path = 'Qwen2.5-3B-Instruct'\n",
    "# 加载模型和分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False, trust_remote_code=True, padding_side=\"right\")\n",
    "\n",
    "from transformers.trainer_pt_utils import LabelSmoother\n",
    "\n",
    "IGNORE_TOKEN_ID = LabelSmoother.ignore_index\n",
    "\n",
    "# 定义预处理数据的逻辑\n",
    "def preprocess(\n",
    "    sources,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    max_len: int,\n",
    "    system_message: str = \"You are a helpful assistant.\"\n",
    "):\n",
    "    \n",
    "    roles = {\"user\": \"<|im_start|>user\", \"assistant\": \"<|im_start|>assistant\"}\n",
    "\n",
    "    im_start = tokenizer('<|im_start|>').input_ids[0]\n",
    "    im_end = tokenizer('<|im_end|>').input_ids[0]\n",
    "    nl_tokens = tokenizer('\\n').input_ids\n",
    "    _system = tokenizer('system').input_ids + nl_tokens\n",
    "    _user = tokenizer('user').input_ids + nl_tokens\n",
    "    _assistant = tokenizer('assistant').input_ids + nl_tokens\n",
    "\n",
    "    # Apply prompt templates\n",
    "    input_ids, targets = [], []\n",
    "    for i, source in enumerate(sources):\n",
    "        if roles[source[0][\"from\"]] != roles[\"user\"]:\n",
    "            source = source[1:]\n",
    "\n",
    "        input_id, target = [], []\n",
    "        system = [im_start] + _system + tokenizer(system_message).input_ids + [im_end] + nl_tokens\n",
    "        input_id += system\n",
    "        target += [im_start] + [IGNORE_TOKEN_ID] * (len(system)-3) + [im_end] + nl_tokens\n",
    "        assert len(input_id) == len(target)\n",
    "        for j, sentence in enumerate(source):\n",
    "            role = roles[sentence[\"from\"]]\n",
    "            _input_id = tokenizer(role).input_ids + nl_tokens + \\\n",
    "                tokenizer(sentence[\"value\"]).input_ids + [im_end] + nl_tokens\n",
    "            input_id += _input_id\n",
    "            if role == '<|im_start|>user':\n",
    "                _target = [im_start] + [IGNORE_TOKEN_ID] * (len(_input_id)-3) + [im_end] + nl_tokens\n",
    "            elif role == '<|im_start|>assistant':\n",
    "                _target = [im_start] + [IGNORE_TOKEN_ID] * len(tokenizer(role).input_ids) + \\\n",
    "                    _input_id[len(tokenizer(role).input_ids)+1:-2] + [im_end] + nl_tokens\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "            target += _target\n",
    "        assert len(input_id) == len(target)\n",
    "        input_id += [tokenizer.pad_token_id] * (max_len - len(input_id))\n",
    "        target += [IGNORE_TOKEN_ID] * (max_len - len(target))\n",
    "        input_ids.append(input_id[:max_len])\n",
    "        targets.append(target[:max_len])\n",
    "    input_ids = torch.tensor(input_ids, dtype=torch.int)\n",
    "    targets = torch.tensor(targets, dtype=torch.int)\n",
    "\n",
    "    return dict(\n",
    "        input_ids=input_ids,\n",
    "        labels=targets,\n",
    "        attention_mask=input_ids.ne(tokenizer.pad_token_id),\n",
    "    )\n",
    "\n",
    "class SupervisedDataset(Dataset):\n",
    "    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    def __init__(self, raw_data, tokenizer, max_len: int):\n",
    "        super(SupervisedDataset, self).__init__()\n",
    "\n",
    "        print(\"Formatting inputs...\")\n",
    "        sources = [example[\"conversations\"] for example in raw_data]\n",
    "        data_dict = preprocess(sources, tokenizer, max_len)\n",
    "\n",
    "        self.input_ids = data_dict[\"input_ids\"]\n",
    "        self.labels = data_dict[\"labels\"]\n",
    "        self.attention_mask = data_dict[\"attention_mask\"]\n",
    "        print(\"Formatting done...\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return dict(\n",
    "            input_ids=self.input_ids[i],\n",
    "            labels=self.labels[i],\n",
    "            attention_mask=self.attention_mask[i],\n",
    "        )\n",
    "\n",
    "# 加载这些数据集\n",
    "train_dataset = SupervisedDataset(format_data_list, tokenizer, max_len=1024)\n",
    "\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设置LoRA参数和量化参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "output_dir = 'checkpoints_self_cong/'\n",
    "# 设置LoRA参数\n",
    "config = LoraConfig(\n",
    "    r=32,                                  # LoRA所使用的Rank\n",
    "    lora_alpha=16,                         # LoRA的Alpha\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],    # LoRA所作用的模块\n",
    "    bias=\"none\",                           # LoRA的Bias，这里设置为none，表示不使用Bias\n",
    "    lora_dropout=0.05,                     # LoRA的Dropout，这里设置为0.05\n",
    "    task_type=\"CAUSAL_LM\",                 \n",
    ")\n",
    "compute_dtype = getattr(torch, \"float16\")\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,                       # 加载4bit量化模型\n",
    "    bnb_4bit_quant_type=\"nf4\",               # 量化类型，这里使用nf4\n",
    "    bnb_4bit_compute_dtype=compute_dtype,    # 计算精度，这里使用float16\n",
    "    bnb_4bit_use_double_quant=True,          # 使用双量化\n",
    ")\n",
    "\n",
    "peft_training_args = TrainingArguments(\n",
    "    output_dir = output_dir,                 # 输出目录\n",
    "    warmup_steps=1,                          # warmup步数，一般在训练模型时，都会将学习率从0逐渐增加到一个较大的值，这个过程就是warmup\n",
    "    per_device_train_batch_size=1,           # 每个设备的训练batch_size\n",
    "    gradient_accumulation_steps=1,           # 梯度累积步数\n",
    "    learning_rate=2e-4,                      # 学习率\n",
    "    optim=\"paged_adamw_8bit\",                # 优化器，这里使用paged_adamw_8bit\n",
    "    logging_steps=100,                       # 多少步打印一次日志\n",
    "    logging_dir=\"./logs\",                    # 日志目录\n",
    "    save_strategy=\"steps\",                   # 保存策略，按照步数保存\n",
    "    max_steps=1000,                          # 要训练多少步\n",
    "    save_steps=500,                          # 多少步保存一次\n",
    "    gradient_checkpointing=True,             # 是否使用gradient_checkpointing功能，这个功能可以节省显存\n",
    "    report_to=\"none\",                        # 不输出报告，这里可以设置成向tensorboard和wandb输出报告\n",
    "    overwrite_output_dir = 'True',           # 是否覆盖输出目录\n",
    "    group_by_length=True,                    # 是否根据长度分组，这个参数可以加速训练，其原理是将长度相近的数据放在一起\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载模型并且开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7657623cc9d4d3482ed090666ac959d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 07:44, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.298800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.507700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.476000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.461900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.292100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.399500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.447100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.365300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.414100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.325100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 466.1714286804199 seconds\n"
     ]
    }
   ],
   "source": [
    "# 加载预训练模型\n",
    "original_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,                                # 预训练模型所存放的路径\n",
    "    torch_dtype=compute_dtype,                 # 模型的精度\n",
    "    quantization_config=quant_config,          # 使用什么样的量化配置\n",
    "    attn_implementation=\"flash_attention_2\",   # 是否要使用flash attention，这里的设置是使用flash attention\n",
    ")\n",
    "\n",
    "# 1 - Enabling gradient checkpointing to reduce memory usage during fine-tuning\n",
    "original_model.gradient_checkpointing_enable()\n",
    "# 2 - Using the prepare_model_for_kbit_training method from PEFT\n",
    "original_model = prepare_model_for_kbit_training(original_model)\n",
    "peft_model = get_peft_model(original_model, config)   # 基于LoRA的配置获取PEFT模型\n",
    "\n",
    "peft_model.config.use_cache = False\n",
    "peft_trainer = Trainer(                   # 定义Trainer\n",
    "    model=peft_model,\n",
    "    train_dataset=train_dataset,\n",
    "    args=peft_training_args,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "start_time = time.time()\n",
    "peft_trainer.train()                      # 开始训练\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Training time: {end_time - start_time} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
